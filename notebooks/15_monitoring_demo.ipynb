{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.config as config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "current_date = pd.Timestamp('2023-01-01 09:00:00')\n",
    "#current_date = pd.to_datetime(datetime.utc.now()).floor('H')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get pointers to the 2 feature groups we need to create our feature view,\n",
    "- One with the model predictions,\n",
    "- The other with the actual demands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/27802\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "Connection closed.\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/27802\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "from src.feature_store_api import get_feature_group\n",
    "predictions_fg = get_feature_group(\n",
    "    name=config.FEATURE_GROUP_MODEL_PREDICTIONS\n",
    ")\n",
    "actuals_fg = get_feature_group(\n",
    "    name=config.FEATURE_GROUP_MODEL_PREDICTIONS\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query to generate our feature view from these feature groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "query = predictions_fg.select_all().join(actuals_fg.select_all(), on=['pickup_hour', 'pickup_location_id']).filter(predictions_fg.pickup_hour >= current_date - timedelta(days=30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection closed.\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/27802\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "Feature view already existed. Skip creation.\n"
     ]
    }
   ],
   "source": [
    "from src.feature_store_api import get_feature_store\n",
    "feature_store = get_feature_store()\n",
    "try:\n",
    "    #create feature view as it does not exist yet\n",
    "    feature_store.create_feature_view(\n",
    "        name=config.FEATURE_VIEW_MONITORING,\n",
    "        version=1,\n",
    "        query=query \n",
    "    )\n",
    "except:\n",
    "    print('Feature view already existed. Skip creation.')\n",
    "\n",
    "#get feature view \n",
    "predictions_and_actuals_fv = feature_store.get_feature_view(\n",
    "    name=config.FEATURE_VIEW_MONITORING,\n",
    "    version=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VersionWarning: No training dataset version was provided to initialise batch scoring . Defaulting to version 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-16 18:42:55,144 INFO: USE `taxi_batch_scoring_featurestore`\n",
      "2023-04-16 18:42:56,058 INFO: WITH right_fg0 AS (SELECT *\n",
      "FROM (SELECT `fg1`.`pickup_location_id` `pickup_location_id`, `fg1`.`predicted_demand` `predicted_demand`, `fg1`.`pickup_hour` `pickup_hour`, `fg1`.`predicted_demand` `predicted_demand`, `fg1`.`pickup_location_id` `join_pk_pickup_location_id`, `fg1`.`pickup_hour` `join_pk_pickup_hour`, `fg1`.`pickup_hour` `join_evt_pickup_hour`, `fg1`.`predicted_demand` `predicted_demand`, `fg1`.`predicted_demand` `predicted_demand`, RANK() OVER (PARTITION BY `fg1`.`pickup_location_id`, `fg1`.`pickup_hour`, `fg1`.`pickup_hour` ORDER BY `fg1`.`pickup_hour` DESC) pit_rank_hopsworks\n",
      "FROM `taxi_batch_scoring_featurestore`.`model_predictions_feature_group_1` `fg1`\n",
      "INNER JOIN `taxi_batch_scoring_featurestore`.`model_predictions_feature_group_1` `fg1` ON `fg1`.`pickup_location_id` = `fg1`.`pickup_location_id` AND `fg1`.`pickup_hour` = `fg1`.`pickup_hour` AND `fg1`.`pickup_hour` >= `fg1`.`pickup_hour`\n",
      "WHERE `fg1`.`pickup_hour` >= TIMESTAMP '2022-12-02 09:00:00.000' AND `fg1`.`pickup_hour` >= TIMESTAMP '2022-12-02 09:00:00.000' AND `fg1`.`pickup_hour` <= TIMESTAMP '2023-01-01 09:00:00.000') NA\n",
      "WHERE `pit_rank_hopsworks` = 1) (SELECT `right_fg0`.`pickup_location_id` `pickup_location_id`, `right_fg0`.`predicted_demand` `predicted_demand`, `right_fg0`.`pickup_hour` `pickup_hour`, `right_fg0`.`predicted_demand` `predicted_demand`, `right_fg0`.`predicted_demand` `predicted_demand`, `right_fg0`.`predicted_demand` `predicted_demand`\n",
      "FROM right_fg0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n"
     ]
    },
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql: WITH right_fg0 AS (SELECT *\nFROM (SELECT `fg1`.`pickup_location_id` `pickup_location_id`, `fg1`.`predicted_demand` `predicted_demand`, `fg1`.`pickup_hour` `pickup_hour`, `fg1`.`predicted_demand` `predicted_demand`, `fg1`.`pickup_location_id` `join_pk_pickup_location_id`, `fg1`.`pickup_hour` `join_pk_pickup_hour`, `fg1`.`pickup_hour` `join_evt_pickup_hour`, `fg1`.`predicted_demand` `predicted_demand`, `fg1`.`predicted_demand` `predicted_demand`, RANK() OVER (PARTITION BY `fg1`.`pickup_location_id`, `fg1`.`pickup_hour`, `fg1`.`pickup_hour` ORDER BY `fg1`.`pickup_hour` DESC) pit_rank_hopsworks\nFROM `taxi_batch_scoring_featurestore`.`model_predictions_feature_group_1` `fg1`\nINNER JOIN `taxi_batch_scoring_featurestore`.`model_predictions_feature_group_1` `fg1` ON `fg1`.`pickup_location_id` = `fg1`.`pickup_location_id` AND `fg1`.`pickup_hour` = `fg1`.`pickup_hour` AND `fg1`.`pickup_hour` >= `fg1`.`pickup_hour`\nWHERE `fg1`.`pickup_hour` >= TIMESTAMP '2022-12-02 09:00:00.000' AND `fg1`.`pickup_hour` >= TIMESTAMP '2022-12-02 09:00:00.000' AND `fg1`.`pickup_hour` <= TIMESTAMP '2023-01-01 09:00:00.000') NA\nWHERE `pit_rank_hopsworks` = 1) (SELECT `right_fg0`.`pickup_location_id` `pickup_location_id`, `right_fg0`.`predicted_demand` `predicted_demand`, `right_fg0`.`pickup_hour` `pickup_hour`, `right_fg0`.`predicted_demand` `predicted_demand`, `right_fg0`.`predicted_demand` `predicted_demand`, `right_fg0`.`predicted_demand` `predicted_demand`\nFROM right_fg0)\nTExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=[\"*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: SemanticException [Error 10008]: Line 4:81 Ambiguous table alias 'fg1':28:27\", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:337', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:203', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:266', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:253', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:541', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatement:HiveSessionImpl.java:516', 'sun.reflect.GeneratedMethodAccessor212:invoke::-1', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:-2', 'javax.security.auth.Subject:doAs:Subject.java:422', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1821', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'com.sun.proxy.$Proxy53:executeStatement::-1', 'org.apache.hive.service.cli.CLIService:executeStatement:CLIService.java:281', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:712', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1557', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1542', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:39', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:286', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1149', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:624', 'java.lang.Thread:run:Thread.java:748', \"*org.apache.hadoop.hive.ql.parse.SemanticException:Line 4:81 Ambiguous table alias 'fg1':49:22\", 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:processTable:SemanticAnalyzer.java:1019', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:processJoin:SemanticAnalyzer.java:1435', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:doPhase1:SemanticAnalyzer.java:1664', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:doPhase1:SemanticAnalyzer.java:1873', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:doPhase1QBExpr:SemanticAnalyzer.java:548', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:processSubQuery:SemanticAnalyzer.java:1142', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:doPhase1:SemanticAnalyzer.java:1658', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:doPhase1:SemanticAnalyzer.java:1873', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:doPhase1QBExpr:SemanticAnalyzer.java:548', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:doPhase1QBExpr:SemanticAnalyzer.java:537', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:gatherCTEReferences:SemanticAnalyzer.java:2018', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:getMaterializationMetadata:SemanticAnalyzer.java:1972', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:getMetaData:SemanticAnalyzer.java:2037', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genResolvedParseTree:SemanticAnalyzer.java:11946', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:analyzeInternal:SemanticAnalyzer.java:12041', 'org.apache.hadoop.hive.ql.parse.CalcitePlanner:analyzeInternal:CalcitePlanner.java:334', 'org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer:analyze:BaseSemanticAnalyzer.java:285', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:643', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1683', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1630', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1625', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:201'], sqlState='42000', errorCode=10008, errorMessage=\"Error while compiling statement: FAILED: SemanticException [Error 10008]: Line 4:81 Ambiguous table alias 'fg1'\"), operationHandle=None)\nunable to rollback",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\gabri\\AppData\\Roaming\\pypoetry\\venv\\lib\\site-packages\\pandas\\io\\sql.py:2018\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2017\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 2018\u001b[0m     cur\u001b[39m.\u001b[39mexecute(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   2019\u001b[0m     \u001b[39mreturn\u001b[39;00m cur\n",
      "File \u001b[1;32mc:\\Users\\gabri\\AppData\\Roaming\\pypoetry\\venv\\lib\\site-packages\\pyhive\\hive.py:408\u001b[0m, in \u001b[0;36mCursor.execute\u001b[1;34m(self, operation, parameters, **kwargs)\u001b[0m\n\u001b[0;32m    407\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connection\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mExecuteStatement(req)\n\u001b[1;32m--> 408\u001b[0m _check_status(response)\n\u001b[0;32m    409\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_operationHandle \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39moperationHandle\n",
      "File \u001b[1;32mc:\\Users\\gabri\\AppData\\Roaming\\pypoetry\\venv\\lib\\site-packages\\pyhive\\hive.py:538\u001b[0m, in \u001b[0;36m_check_status\u001b[1;34m(response)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus\u001b[39m.\u001b[39mstatusCode \u001b[39m!=\u001b[39m ttypes\u001b[39m.\u001b[39mTStatusCode\u001b[39m.\u001b[39mSUCCESS_STATUS:\n\u001b[1;32m--> 538\u001b[0m     \u001b[39mraise\u001b[39;00m OperationalError(response)\n",
      "\u001b[1;31mOperationalError\u001b[0m: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=[\"*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: SemanticException [Error 10008]: Line 4:81 Ambiguous table alias 'fg1':28:27\", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:337', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:203', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:266', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:253', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:541', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatement:HiveSessionImpl.java:516', 'sun.reflect.GeneratedMethodAccessor212:invoke::-1', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:-2', 'javax.security.auth.Subject:doAs:Subject.java:422', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1821', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'com.sun.proxy.$Proxy53:executeStatement::-1', 'org.apache.hive.service.cli.CLIService:executeStatement:CLIService.java:281', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:712', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1557', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1542', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:39', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:286', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1149', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:624', 'java.lang.Thread:run:Thread.java:748', \"*org.apache.hadoop.hive.ql.parse.SemanticException:Line 4:81 Ambiguous table alias 'fg1':49:22\", 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:processTable:SemanticAnalyzer.java:1019', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:processJoin:SemanticAnalyzer.java:1435', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:doPhase1:SemanticAnalyzer.java:1664', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:doPhase1:SemanticAnalyzer.java:1873', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:doPhase1QBExpr:SemanticAnalyzer.java:548', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:processSubQuery:SemanticAnalyzer.java:1142', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:doPhase1:SemanticAnalyzer.java:1658', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:doPhase1:SemanticAnalyzer.java:1873', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:doPhase1QBExpr:SemanticAnalyzer.java:548', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:doPhase1QBExpr:SemanticAnalyzer.java:537', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:gatherCTEReferences:SemanticAnalyzer.java:2018', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:getMaterializationMetadata:SemanticAnalyzer.java:1972', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:getMetaData:SemanticAnalyzer.java:2037', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genResolvedParseTree:SemanticAnalyzer.java:11946', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:analyzeInternal:SemanticAnalyzer.java:12041', 'org.apache.hadoop.hive.ql.parse.CalcitePlanner:analyzeInternal:CalcitePlanner.java:334', 'org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer:analyze:BaseSemanticAnalyzer.java:285', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:643', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1683', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1630', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1625', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:201'], sqlState='42000', errorCode=10008, errorMessage=\"Error while compiling statement: FAILED: SemanticException [Error 10008]: Line 4:81 Ambiguous table alias 'fg1'\"), operationHandle=None)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNotSupportedError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\gabri\\AppData\\Roaming\\pypoetry\\venv\\lib\\site-packages\\pandas\\io\\sql.py:2022\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2021\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 2022\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcon\u001b[39m.\u001b[39;49mrollback()\n\u001b[0;32m   2023\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m inner_exc:  \u001b[39m# pragma: no cover\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gabri\\AppData\\Roaming\\pypoetry\\venv\\lib\\site-packages\\pyhive\\hive.py:285\u001b[0m, in \u001b[0;36mConnection.rollback\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrollback\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 285\u001b[0m     \u001b[39mraise\u001b[39;00m NotSupportedError(\u001b[39m\"\u001b[39m\u001b[39mHive does not have transactions\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNotSupportedError\u001b[0m: Hive does not have transactions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#fetch predicted and actual values for the last 30 days\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m preds_and_actuals \u001b[39m=\u001b[39m predictions_and_actuals_fv\u001b[39m.\u001b[39;49mget_batch_data(\n\u001b[0;32m      3\u001b[0m     start_time \u001b[39m=\u001b[39;49m (current_date \u001b[39m-\u001b[39;49m timedelta(days\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m)),\n\u001b[0;32m      4\u001b[0m     end_time \u001b[39m=\u001b[39;49m current_date\n\u001b[0;32m      5\u001b[0m )\n\u001b[0;32m      7\u001b[0m preds_and_actuals \u001b[39m=\u001b[39m preds_and_actuals[preds_and_actuals\u001b[39m.\u001b[39mpickup_hour\u001b[39m.\u001b[39mbetween(current_date \u001b[39m-\u001b[39m timedelta(days\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m), current_date)]\n\u001b[0;32m      8\u001b[0m preds_and_actuals \n",
      "File \u001b[1;32mc:\\Users\\gabri\\AppData\\Roaming\\pypoetry\\venv\\lib\\site-packages\\hsfs\\feature_view.py:301\u001b[0m, in \u001b[0;36mFeatureView.get_batch_data\u001b[1;34m(self, start_time, end_time, read_options)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_scoring_server \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    299\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_batch_scoring()\n\u001b[1;32m--> 301\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_feature_view_engine\u001b[39m.\u001b[39;49mget_batch_data(\n\u001b[0;32m    302\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[0;32m    303\u001b[0m     start_time,\n\u001b[0;32m    304\u001b[0m     end_time,\n\u001b[0;32m    305\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_scoring_server\u001b[39m.\u001b[39;49mtraining_dataset_version,\n\u001b[0;32m    306\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_scoring_server\u001b[39m.\u001b[39;49m_transformation_functions,\n\u001b[0;32m    307\u001b[0m     read_options,\n\u001b[0;32m    308\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\gabri\\AppData\\Roaming\\pypoetry\\venv\\lib\\site-packages\\hsfs\\core\\feature_view_engine.py:397\u001b[0m, in \u001b[0;36mFeatureViewEngine.get_batch_data\u001b[1;34m(self, feature_view_obj, start_time, end_time, training_dataset_version, transformation_functions, read_options)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_batch_data\u001b[39m(\n\u001b[0;32m    387\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    388\u001b[0m     feature_view_obj,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m     read_options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    394\u001b[0m ):\n\u001b[0;32m    395\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_feature_group_accessibility(feature_view_obj)\n\u001b[1;32m--> 397\u001b[0m     feature_dataframe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_batch_query(\n\u001b[0;32m    398\u001b[0m         feature_view_obj, start_time, end_time, with_label\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m    399\u001b[0m     )\u001b[39m.\u001b[39;49mread(read_options\u001b[39m=\u001b[39;49mread_options)\n\u001b[0;32m    401\u001b[0m     training_dataset_obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_training_data_metadata(\n\u001b[0;32m    402\u001b[0m         feature_view_obj, training_dataset_version\n\u001b[0;32m    403\u001b[0m     )\n\u001b[0;32m    404\u001b[0m     training_dataset_obj\u001b[39m.\u001b[39mtransformation_functions \u001b[39m=\u001b[39m transformation_functions\n",
      "File \u001b[1;32mc:\\Users\\gabri\\AppData\\Roaming\\pypoetry\\venv\\lib\\site-packages\\hsfs\\constructor\\query.py:108\u001b[0m, in \u001b[0;36mQuery.read\u001b[1;34m(self, online, dataframe_type, read_options)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Read the specified query into a DataFrame.\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \n\u001b[0;32m     86\u001b[0m \u001b[39mIt is possible to specify the storage (online/offline) to read from and the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m    `DataFrame`: DataFrame depending on the chosen type.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    106\u001b[0m sql_query, online_conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prep_read(online, read_options)\n\u001b[1;32m--> 108\u001b[0m \u001b[39mreturn\u001b[39;00m engine\u001b[39m.\u001b[39;49mget_instance()\u001b[39m.\u001b[39;49msql(\n\u001b[0;32m    109\u001b[0m     sql_query,\n\u001b[0;32m    110\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_feature_store_name,\n\u001b[0;32m    111\u001b[0m     online_conn,\n\u001b[0;32m    112\u001b[0m     dataframe_type,\n\u001b[0;32m    113\u001b[0m     read_options,\n\u001b[0;32m    114\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\gabri\\AppData\\Roaming\\pypoetry\\venv\\lib\\site-packages\\hsfs\\engine\\python.py:84\u001b[0m, in \u001b[0;36mEngine.sql\u001b[1;34m(self, sql_query, feature_store, online_conn, dataframe_type, read_options)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msql\u001b[39m(\u001b[39mself\u001b[39m, sql_query, feature_store, online_conn, dataframe_type, read_options):\n\u001b[0;32m     83\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m online_conn:\n\u001b[1;32m---> 84\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sql_offline(sql_query, feature_store, dataframe_type)\n\u001b[0;32m     85\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jdbc(sql_query, online_conn, dataframe_type, read_options)\n",
      "File \u001b[1;32mc:\\Users\\gabri\\AppData\\Roaming\\pypoetry\\venv\\lib\\site-packages\\hsfs\\engine\\python.py:90\u001b[0m, in \u001b[0;36mEngine._sql_offline\u001b[1;34m(self, sql_query, feature_store, dataframe_type)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sql_offline\u001b[39m(\u001b[39mself\u001b[39m, sql_query, feature_store, dataframe_type):\n\u001b[0;32m     89\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_hive_connection(feature_store) \u001b[39mas\u001b[39;00m hive_conn:\n\u001b[1;32m---> 90\u001b[0m         result_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_sql(sql_query, hive_conn)\n\u001b[0;32m     91\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_dataframe_type(result_df, dataframe_type)\n",
      "File \u001b[1;32mc:\\Users\\gabri\\AppData\\Roaming\\pypoetry\\venv\\lib\\site-packages\\pandas\\io\\sql.py:564\u001b[0m, in \u001b[0;36mread_sql\u001b[1;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize)\u001b[0m\n\u001b[0;32m    561\u001b[0m pandas_sql \u001b[39m=\u001b[39m pandasSQL_builder(con)\n\u001b[0;32m    563\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(pandas_sql, SQLiteDatabase):\n\u001b[1;32m--> 564\u001b[0m     \u001b[39mreturn\u001b[39;00m pandas_sql\u001b[39m.\u001b[39;49mread_query(\n\u001b[0;32m    565\u001b[0m         sql,\n\u001b[0;32m    566\u001b[0m         index_col\u001b[39m=\u001b[39;49mindex_col,\n\u001b[0;32m    567\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    568\u001b[0m         coerce_float\u001b[39m=\u001b[39;49mcoerce_float,\n\u001b[0;32m    569\u001b[0m         parse_dates\u001b[39m=\u001b[39;49mparse_dates,\n\u001b[0;32m    570\u001b[0m         chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[0;32m    571\u001b[0m     )\n\u001b[0;32m    573\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    574\u001b[0m     _is_table_name \u001b[39m=\u001b[39m pandas_sql\u001b[39m.\u001b[39mhas_table(sql)\n",
      "File \u001b[1;32mc:\\Users\\gabri\\AppData\\Roaming\\pypoetry\\venv\\lib\\site-packages\\pandas\\io\\sql.py:2078\u001b[0m, in \u001b[0;36mSQLiteDatabase.read_query\u001b[1;34m(self, sql, index_col, coerce_float, params, parse_dates, chunksize, dtype)\u001b[0m\n\u001b[0;32m   2066\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_query\u001b[39m(\n\u001b[0;32m   2067\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   2068\u001b[0m     sql,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2074\u001b[0m     dtype: DtypeArg \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   2075\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Iterator[DataFrame]:\n\u001b[0;32m   2077\u001b[0m     args \u001b[39m=\u001b[39m _convert_params(sql, params)\n\u001b[1;32m-> 2078\u001b[0m     cursor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m   2079\u001b[0m     columns \u001b[39m=\u001b[39m [col_desc[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m col_desc \u001b[39min\u001b[39;00m cursor\u001b[39m.\u001b[39mdescription]\n\u001b[0;32m   2081\u001b[0m     \u001b[39mif\u001b[39;00m chunksize \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\gabri\\AppData\\Roaming\\pypoetry\\venv\\lib\\site-packages\\pandas\\io\\sql.py:2027\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2023\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m inner_exc:  \u001b[39m# pragma: no cover\u001b[39;00m\n\u001b[0;32m   2024\u001b[0m     ex \u001b[39m=\u001b[39m DatabaseError(\n\u001b[0;32m   2025\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExecution failed on sql: \u001b[39m\u001b[39m{\u001b[39;00margs[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mexc\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39munable to rollback\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2026\u001b[0m     )\n\u001b[1;32m-> 2027\u001b[0m     \u001b[39mraise\u001b[39;00m ex \u001b[39mfrom\u001b[39;00m \u001b[39minner_exc\u001b[39;00m\n\u001b[0;32m   2029\u001b[0m ex \u001b[39m=\u001b[39m DatabaseError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExecution failed on sql \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00margs[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mexc\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   2030\u001b[0m \u001b[39mraise\u001b[39;00m ex \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n",
      "\u001b[1;31mDatabaseError\u001b[0m: Execution failed on sql: WITH right_fg0 AS (SELECT *\nFROM (SELECT `fg1`.`pickup_location_id` `pickup_location_id`, `fg1`.`predicted_demand` `predicted_demand`, `fg1`.`pickup_hour` `pickup_hour`, `fg1`.`predicted_demand` `predicted_demand`, `fg1`.`pickup_location_id` `join_pk_pickup_location_id`, `fg1`.`pickup_hour` `join_pk_pickup_hour`, `fg1`.`pickup_hour` `join_evt_pickup_hour`, `fg1`.`predicted_demand` `predicted_demand`, `fg1`.`predicted_demand` `predicted_demand`, RANK() OVER (PARTITION BY `fg1`.`pickup_location_id`, `fg1`.`pickup_hour`, `fg1`.`pickup_hour` ORDER BY `fg1`.`pickup_hour` DESC) pit_rank_hopsworks\nFROM `taxi_batch_scoring_featurestore`.`model_predictions_feature_group_1` `fg1`\nINNER JOIN `taxi_batch_scoring_featurestore`.`model_predictions_feature_group_1` `fg1` ON `fg1`.`pickup_location_id` = `fg1`.`pickup_location_id` AND `fg1`.`pickup_hour` = `fg1`.`pickup_hour` AND `fg1`.`pickup_hour` >= `fg1`.`pickup_hour`\nWHERE `fg1`.`pickup_hour` >= TIMESTAMP '2022-12-02 09:00:00.000' AND `fg1`.`pickup_hour` >= TIMESTAMP '2022-12-02 09:00:00.000' AND `fg1`.`pickup_hour` <= TIMESTAMP '2023-01-01 09:00:00.000') NA\nWHERE `pit_rank_hopsworks` = 1) (SELECT `right_fg0`.`pickup_location_id` `pickup_location_id`, `right_fg0`.`predicted_demand` `predicted_demand`, `right_fg0`.`pickup_hour` `pickup_hour`, `right_fg0`.`predicted_demand` `predicted_demand`, `right_fg0`.`predicted_demand` `predicted_demand`, `right_fg0`.`predicted_demand` `predicted_demand`\nFROM right_fg0)\nTExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=[\"*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: SemanticException [Error 10008]: Line 4:81 Ambiguous table alias 'fg1':28:27\", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:337', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:203', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:266', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:253', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:541', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatement:HiveSessionImpl.java:516', 'sun.reflect.GeneratedMethodAccessor212:invoke::-1', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:-2', 'javax.security.auth.Subject:doAs:Subject.java:422', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1821', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'com.sun.proxy.$Proxy53:executeStatement::-1', 'org.apache.hive.service.cli.CLIService:executeStatement:CLIService.java:281', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:712', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1557', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1542', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:39', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:286', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1149', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:624', 'java.lang.Thread:run:Thread.java:748', \"*org.apache.hadoop.hive.ql.parse.SemanticException:Line 4:81 Ambiguous table alias 'fg1':49:22\", 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:processTable:SemanticAnalyzer.java:1019', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:processJoin:SemanticAnalyzer.java:1435', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:doPhase1:SemanticAnalyzer.java:1664', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:doPhase1:SemanticAnalyzer.java:1873', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:doPhase1QBExpr:SemanticAnalyzer.java:548', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:processSubQuery:SemanticAnalyzer.java:1142', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:doPhase1:SemanticAnalyzer.java:1658', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:doPhase1:SemanticAnalyzer.java:1873', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:doPhase1QBExpr:SemanticAnalyzer.java:548', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:doPhase1QBExpr:SemanticAnalyzer.java:537', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:gatherCTEReferences:SemanticAnalyzer.java:2018', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:getMaterializationMetadata:SemanticAnalyzer.java:1972', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:getMetaData:SemanticAnalyzer.java:2037', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genResolvedParseTree:SemanticAnalyzer.java:11946', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:analyzeInternal:SemanticAnalyzer.java:12041', 'org.apache.hadoop.hive.ql.parse.CalcitePlanner:analyzeInternal:CalcitePlanner.java:334', 'org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer:analyze:BaseSemanticAnalyzer.java:285', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:643', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1683', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1630', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1625', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:201'], sqlState='42000', errorCode=10008, errorMessage=\"Error while compiling statement: FAILED: SemanticException [Error 10008]: Line 4:81 Ambiguous table alias 'fg1'\"), operationHandle=None)\nunable to rollback"
     ]
    }
   ],
   "source": [
    "#fetch predicted and actual values for the last 30 days\n",
    "preds_and_actuals = predictions_and_actuals_fv.get_batch_data(\n",
    "    start_time = (current_date - timedelta(days=30)),\n",
    "    end_time = current_date\n",
    ")\n",
    "\n",
    "preds_and_actuals = preds_and_actuals[preds_and_actuals.pickup_hour.between(current_date - timedelta(days=30), current_date)]\n",
    "preds_and_actuals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = predictions_and_actuals\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
